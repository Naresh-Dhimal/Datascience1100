{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Prrocessing"
      ],
      "metadata": {
        "id": "vNx1gpMH5GLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f6JcsJEq44T6"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text preprocessing and cleaning"
      ],
      "metadata": {
        "id": "7PtEZVb35BbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is the task of splitting a text into meaningful segments, called tokens.\""
      ],
      "metadata": {
        "id": "gcBul6my5aAo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "jAbX2_aU5mIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word tokenization"
      ],
      "metadata": {
        "id": "zy3-U72J5pXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "WMKskLDW51LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0icFo3J455pN",
        "outputId": "abf39186-0e3c-4b10-965c-72c6b616174c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenization',\n",
              " 'is',\n",
              " 'the',\n",
              " 'task',\n",
              " 'of',\n",
              " 'splitting',\n",
              " 'a',\n",
              " 'text',\n",
              " 'into',\n",
              " 'meaningful',\n",
              " 'segments',\n",
              " ',',\n",
              " 'called',\n",
              " 'tokens',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu_ENaz16ICx",
        "outputId": "d307ab39-1631-4e2f-83ff-60336b9859a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenization',\n",
              " 'is',\n",
              " 'the',\n",
              " 'task',\n",
              " 'of',\n",
              " 'splitting',\n",
              " 'a',\n",
              " 'text',\n",
              " 'into',\n",
              " 'meaningful',\n",
              " 'segments',\n",
              " ',',\n",
              " 'called',\n",
              " 'tokens',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sentence tokenization"
      ],
      "metadata": {
        "id": "ivLBrU8P6Q6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph =\"He is a good boy. she is a good girl.\""
      ],
      "metadata": {
        "id": "QHJTnLb56Yur"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJbMYT76gaU",
        "outputId": "36d61d96-fb13-44a1-b1f2-2cd83e4f847a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He is a good boy.', 'she is a good girl.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = nltk.sent_tokenize(paragraph)\n",
        "para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l41Y7Fsr6lTC",
        "outputId": "1d6bd672-5c1d-4ab2-9ca4-3e39eff32027"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He is a good boy.', 'she is a good girl.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords and stopsword removal.\n",
        "*  words like \"is, the, of,a \" is not much important because it does not much meaning to the sentence we can remove them using stopsword"
      ],
      "metadata": {
        "id": "lGAXnh6L6qwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords in english languages\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "stopword"
      ],
      "metadata": {
        "id": "BGa5-Gdj7WxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "stopword"
      ],
      "metadata": {
        "id": "I3mUW8Wf7tVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords in nepali languages\n",
        "stopword = nltk.corpus.stopwords.words('nepali')\n",
        "stopword"
      ],
      "metadata": {
        "id": "TYIURq6U7yW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stopwords removal"
      ],
      "metadata": {
        "id": "wbxd8zB7759X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"He is a good boy\""
      ],
      "metadata": {
        "id": "OEWfBtLY8DVN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sent)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfGkU__J8Fyj",
        "outputId": "357799c7-429c-4003-d12e-8b7bad3f606a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He', 'is', 'a', 'good', 'boy']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VkF8UiX8LNU",
        "outputId": "ba6545fe-4f91-41cf-f4d3-75b9200591bf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = [word for word in words if word.lower() not in stopwords]\n",
        "\" \".join(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_us8ntcF8Uk3",
        "outputId": "701273df-ed31-473a-8451-03ddfd7ff381"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good boy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming: convert words into root word or base word stem or base word - word may not have any meaning - very fast"
      ],
      "metadata": {
        "id": "x7QYjFIQ8uS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['change', 'changes', \"changing\"]\n"
      ],
      "metadata": {
        "id": "X6jcqJuq9nGU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "Cgc3q94N-FxV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(word) for word in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkM75g6H-ME5",
        "outputId": "b3fe8f79-ca4e-44ec-86e9-38c64472fbf4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chang', 'chang', 'chang']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### chang ko dictionary meaning chaina"
      ],
      "metadata": {
        "id": "jsPz996b-WpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization: Lemmatization deals with reducing the word to its canonical dictionary form. The root word is called a \"lemma\" and the method is called lemmatization.-slow"
      ],
      "metadata": {
        "id": "lUgYyMHn-h1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjac9dCRADet",
        "outputId": "ef76829e-ca53-4be7-df53-10549c9ad710"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['change', 'changes','changing','changed']\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLkImHsg_r2r",
        "outputId": "b507312a-cbee-41f1-d127-ae36c650b8ab"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['change', 'change', 'changing', 'changed']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing HTML from text"
      ],
      "metadata": {
        "id": "yxr5X3tVABAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"<p><b>Pangolins</b> risk extinction in Khotang</p>\""
      ],
      "metadata": {
        "id": "VeOaxrG_A96s"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DBpsSeSbBaUH",
        "outputId": "281e9e91-3376-430f-813a-c90f237506ca"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p><b>Pangolins</b> risk extinction in Khotang</p>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(text, \"html.parser\").text"
      ],
      "metadata": {
        "id": "hSUzIMmuBbAx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ECdV2jveBkcH",
        "outputId": "e8f144a2-db7e-43c2-a1ec-8c9c680553a5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pangolins risk extinction in Khotang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex or Regular Expression\n",
        "A regular expression, sometimes referred to as rational expression, is a sequence of characters that specifies a match pattern in text. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation."
      ],
      "metadata": {
        "id": "fAFkNHunB5Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "F8XdoUJVCATE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = \"a....s\"\n",
        "text = \"abacus\"\n",
        "re.match(pattern, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HoFg9gdCPfG",
        "outputId": "7cbf0a3e-c6b7-485f-d03e-31b9d1e226d5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abacus'>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "pattern = \"a....s\"\n",
        "text = \"abas\"\n",
        "re.match(pattern, text)"
      ],
      "metadata": {
        "id": "NDCc6O4LChoW"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ ]\n",
        "* specifies set of character you widh to match"
      ],
      "metadata": {
        "id": "_5mjohGwC288"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = '[abc]'\n",
        "string = 'aklbac'\n",
        "re.match(pattern, string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2n4HdJmDRnP",
        "outputId": "502adb66-48e3-4093-b593-5d74399a3431"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = '[abc]'\n",
        "string = 'aklbac'\n",
        "re.findall(pattern, string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4vlxAQwDdtS",
        "outputId": "9dc4d4c4-22bd-4a71-e9ed-ba494fcae726"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b', 'a', 'c']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = \"[0-9]\"\n",
        "string = \"Hello world I am naresh dhimal. Currently, i am 24 years old.\"\n",
        "re.findall(pattern, string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlhxOQ1CEJ2A",
        "outputId": "3082a595-9f4f-4f46-9540-12db92603585"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2', '4']"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = \"\\d+\"\n",
        "string = \"Hello world I am naresh dhimal. Currently, i am 24 years old.\"\n",
        "re.findall(pattern, string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KfrwseIEaNF",
        "outputId": "5597dfc9-8deb-4c1b-e252-a6ed9de04b8a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['24']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# +"
      ],
      "metadata": {
        "id": "l6RcTMrpEg4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = \"[ab]+\"\n",
        "string = \"abibas\"\n",
        "re.findall(pattern, string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ2lVTdSEo0_",
        "outputId": "005ec799-a4b0-4143-9d6a-4e37178ef49f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ab', 'ba']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern= r'\\bfoo'\n",
        "string = 'football, foot , foo '\n",
        "re.findall(pattern, string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK4yNcqDFKEF",
        "outputId": "ab0253ba-e10c-4e3b-b6c8-658e40394023"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['foo', 'foo', 'foo']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "email= \"\"\"\n",
        "naresh@gmail.com\n",
        "info@gmail.com\n",
        "ram.sigh@gmail.com\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jxKNpfTgFbVk"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern= r'\\b[0-9a-zA-Z]+\\@gmail.com\\b'"
      ],
      "metadata": {
        "id": "BaNRP8FaHLP-"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(pattern, email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80-1p61iHNM9",
        "outputId": "31ad515a-c760-4fe8-bf19-a783f819c58d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['naresh@gmail.com', 'info@gmail.com', 'sigh@gmail.com']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove html using regex"
      ],
      "metadata": {
        "id": "EAVJjZT8GHz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"<p><b>Pangolins</b> risk extinction in Khotang</p>\""
      ],
      "metadata": {
        "id": "rOUVil2GG8_a"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  re\n",
        "CLEANER = re.compile('<.*?>')\n",
        "def clean_html(raw_html):\n",
        "  cleantext = re.sub(CLEANER, \"\", raw_html)\n",
        "  return cleantext"
      ],
      "metadata": {
        "id": "sLGD4q-GHAJn"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_html(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JW5cJMzLHjTG",
        "outputId": "59334444-9e60-47d6-f0d8-96b7cbfc0ce5"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pangolins risk extinction in Khotang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conerting text to vector\n",
        "* Bag of words/ CountVectorizer()\n",
        "* Tfidf-vectorizer"
      ],
      "metadata": {
        "id": "60fwirpLHnqB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1MSsmCOHr7P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}